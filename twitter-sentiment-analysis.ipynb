{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30075,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> <img src=\"https://miro.medium.com/max/700/1*0OVev9mGkNJblfkOxkknAQ.png\"> </center>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n<h1><strong>Introduction</strong></h1>\n    <p>Millions of people are using Twitter and expressing their emotions like happiness, sadness, angry, etc. The Sentiment analysis is also about detecting the emotions, opinion, assessment, attitudes, and took this into consideration as a way humans think. Sentiment analysis classifies the emotions into classes such as positive or negative. Nowadays, industries are interested to use¬†textual data for semantic analysis to extract the view of people about their products and services. Sentiment analysis is very important for them to know the customer satisfaction level and they can improve their services accordingly. To work on the text data, they try to extract the data from social media platforms. There are a lot of social media sites like Google Plus, Facebook, and Twitter that allow expressing opinions, views, and emotions about certain topics and events. Microblogging site Twitter is expanding rapidly among all other online social media networking sites with about 200 million users. Twitter was founded in 2006 and currently, it is the most famous microblogging platform. In 2017 2 million users shared 8.3 million tweets in one hour. Twitter users use to post their thoughts, emotions, and messages on their profiles, called tweets. Words limit of a single tweet has 140 characters. Twitter sentiment analysis based on the NLP (natural language processing) field. For tweets text, we use NLP techniques like tokenizing the words, removing the stop words like I, me, my, our, your, is, was, etc. Natural language processing also plays a part to preprocess the data like cleaning the text and removing the special characters and punctuation marks. Sentimental analysis is very important because we can know the trends of people‚Äôs emotions on specific topics with their tweets.</p>\n    <br>\n        <hr>\n      <b>Problem description/definition: </b> \n    <hr>\n<ul>\n    <li>To devise a sentimental analyzer for overcoming the challenges to identify the twitter tweets text sentiments (positive, negative) by implementing neural network using tensorflow</li>\n</ul>\n\n\n<hr>\n<b>Evolution measures: </b> \n<hr>\n<ul>\n<p> After training the model, we apply the evaluation measures to check that how the model is getting predictions. We will use the following evaluation measures to evaluate the performance of the models:</p>\n    <li>Accuracy</li>\n    <li>Confusion matrix with plot</li>\n    <li>ROC Curve</li>\n</ul>\n<hr>\n<b>Technical Approach</b>\n<hr>\n<p>We are using python language in the implementations and Jupter Notebook that support the machine learning and data science projects. We will build tensorflow based model. We will use Sentiment 140 dataset and split that data into 70% for training and 30% for the testing purposes. After training on the model, we will evaluate the model to evaluate the performance of trained model</p>\n \n<hr>\n<b>Source of Data: </b> \n<hr> \n <a href=\"https://www.kaggle.com/kazanova/sentiment140\">https://www.kaggle.com/kazanova/sentiment140</a>\n   \n</div>","metadata":{"papermill":{"duration":0.020638,"end_time":"2020-12-02T21:20:36.560772","exception":false,"start_time":"2020-12-02T21:20:36.540134","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Importing Python Libraries üìï üìó üìò üìô</strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"- Libraries are important and we call them to perform the different actions on our data and for training the models.\n- Its a first step to load the library to perform the specific task","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport string\nfrom tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"-E4TvIVLMa73","scrolled":true,"execution":{"iopub.status.busy":"2023-04-03T08:06:57.321911Z","iopub.execute_input":"2023-04-03T08:06:57.322357Z","iopub.status.idle":"2023-04-03T08:06:59.98071Z","shell.execute_reply.started":"2023-04-03T08:06:57.322267Z","shell.execute_reply":"2023-04-03T08:06:59.979609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> We are uisng the following versions of the libraries:</h4>\n\n- numpy == 1.18.5 \n\n- pandas == 1.1.3\n\n- tensorflow ==1.7.0\n\n- keras == 2.4.3\n\n- nltk ==3.5\n\n- seaborn ==0.11.0","metadata":{}},{"cell_type":"markdown","source":"<h4>How we can install the libraries in python?</h4>","metadata":{}},{"cell_type":"markdown","source":"<h4>To install the python library is very easy</h4>\n- pip install name_of_library \n<h5> Like if you wanted to install tensorflow? </h5>\n- pip install tensforflow","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Loading the data üìÅ üìÇ</strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\ndata.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:06:59.983151Z","iopub.execute_input":"2023-04-03T08:06:59.983639Z","iopub.status.idle":"2023-04-03T08:07:13.73835Z","shell.execute_reply.started":"2023-04-03T08:06:59.983591Z","shell.execute_reply":"2023-04-03T08:07:13.73733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Exploratory data analysis üîé üìä</strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Five top records of data","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.740119Z","iopub.execute_input":"2023-04-03T08:07:13.740591Z","iopub.status.idle":"2023-04-03T08:07:13.762702Z","shell.execute_reply.started":"2023-04-03T08:07:13.740542Z","shell.execute_reply":"2023-04-03T08:07:13.761647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Five last records of data","metadata":{}},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.764636Z","iopub.execute_input":"2023-04-03T08:07:13.764971Z","iopub.status.idle":"2023-04-03T08:07:13.780465Z","shell.execute_reply.started":"2023-04-03T08:07:13.764937Z","shell.execute_reply":"2023-04-03T08:07:13.779237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Coloumns/features in data","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.784219Z","iopub.execute_input":"2023-04-03T08:07:13.784702Z","iopub.status.idle":"2023-04-03T08:07:13.792982Z","shell.execute_reply.started":"2023-04-03T08:07:13.784653Z","shell.execute_reply":"2023-04-03T08:07:13.791936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Length of data","metadata":{}},{"cell_type":"code","source":"print('lenght of data is', len(data))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.796678Z","iopub.execute_input":"2023-04-03T08:07:13.797052Z","iopub.status.idle":"2023-04-03T08:07:13.803593Z","shell.execute_reply.started":"2023-04-03T08:07:13.797017Z","shell.execute_reply":"2023-04-03T08:07:13.802494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shape of data","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.805374Z","iopub.execute_input":"2023-04-03T08:07:13.805803Z","iopub.status.idle":"2023-04-03T08:07:13.814158Z","shell.execute_reply.started":"2023-04-03T08:07:13.805768Z","shell.execute_reply":"2023-04-03T08:07:13.813064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data information","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:13.815243Z","iopub.execute_input":"2023-04-03T08:07:13.815585Z","iopub.status.idle":"2023-04-03T08:07:14.392341Z","shell.execute_reply.started":"2023-04-03T08:07:13.815553Z","shell.execute_reply":"2023-04-03T08:07:14.391313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data types of all coloumns","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:14.393621Z","iopub.execute_input":"2023-04-03T08:07:14.393912Z","iopub.status.idle":"2023-04-03T08:07:14.402041Z","shell.execute_reply.started":"2023-04-03T08:07:14.393884Z","shell.execute_reply":"2023-04-03T08:07:14.400877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Null values","metadata":{}},{"cell_type":"code","source":"np.sum(data.isnull().any(axis=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-04-03T08:07:14.403805Z","iopub.execute_input":"2023-04-03T08:07:14.404144Z","iopub.status.idle":"2023-04-03T08:07:14.972744Z","shell.execute_reply.started":"2023-04-03T08:07:14.40411Z","shell.execute_reply":"2023-04-03T08:07:14.971602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rows and columns in the dataset","metadata":{}},{"cell_type":"code","source":"print('Count of columns in the data is:  ', len(data.columns))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:14.974572Z","iopub.execute_input":"2023-04-03T08:07:14.974909Z","iopub.status.idle":"2023-04-03T08:07:14.981879Z","shell.execute_reply.started":"2023-04-03T08:07:14.974875Z","shell.execute_reply":"2023-04-03T08:07:14.980766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Count of rows in the data is:  ', len(data))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:14.983366Z","iopub.execute_input":"2023-04-03T08:07:14.983888Z","iopub.status.idle":"2023-04-03T08:07:14.993506Z","shell.execute_reply.started":"2023-04-03T08:07:14.983832Z","shell.execute_reply":"2023-04-03T08:07:14.992527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Data Preparation üìù</strong></center></h1>\n\n <ul style=\"list-style-type:circle;\">\n     <h6>\n         <li>Selection of interested coloumns</li>\n         <br>\n          <li>Assinged 1 to class 4</li>\n         <br>\n          <li>Took one fourth data so we can run on our machine easily</li>\n         <br>\n          <li>Combined positive and negative tweets</li>\n         <br>\n    <li>We will convert the text in lower case for the further working on tweet text. </li>\n         <br>\n         <li>We will clean and remove the stop words(of, a, in etc) from statement because these words are not useuseful to support the labels of sentiments  data</li>\n<br>\n         <li>We will clean and remove the punctuations because these are the noise in the data and not meaningfull</li>\n         <br>\n         <li>We will clean and remove repeating characters in the words</li>\n         <br>\n         <li>We will clean and remove emails</li>\n         <br>\n         <li>We will clean and remove URL's</li>\n         <br>\n         <li>We will clean and remove the numbers in the data</li>\n         <br>\n         <li>We will apply tokenization(to separate the sentence into words)</li>\n         <br>\n         <li>We will apply stemming and lemmatization on the text. The concept of both is following:</li>\n         <img src=\"https://lh3.googleusercontent.com/3wumK8lGLhKpD2Fhbu35I7wWf6OSpF_erX9T7FX9WQCE5_HBKMJpKOZNximlzlTG5882QUWcL-_lFLJd0-RIo4uHDaO7cK8aEnw2Tm2-5xPwjYS3ls6fYefeGAVGb1WUGrXafJrC\">\n          <br>\n         <li>We will apply stemming on the tweet text.</li>\n         <br>\n         <li>We will apply lemmatization on the tweet text.</li>\n         <br>\n         <li>Separated input feature and labels</li>\n         <br>\n         <li>Extracted features from input feature</li>\n         <br>\n         <li>Separated the 70% data for training and 30% data for testing</li>\n</h6>\n</ul>\n   \n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Selecting the text and label coloumn","metadata":{}},{"cell_type":"code","source":"data=data[['text','label']]","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:14.995782Z","iopub.execute_input":"2023-04-03T08:07:14.996112Z","iopub.status.idle":"2023-04-03T08:07:15.040576Z","shell.execute_reply.started":"2023-04-03T08:07:14.99608Z","shell.execute_reply":"2023-04-03T08:07:15.039479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Assigning 1 to Positive sentment 4","metadata":{}},{"cell_type":"code","source":"data['label'][data['label']==4]=1","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.041931Z","iopub.execute_input":"2023-04-03T08:07:15.042427Z","iopub.status.idle":"2023-04-03T08:07:15.066103Z","shell.execute_reply.started":"2023-04-03T08:07:15.04239Z","shell.execute_reply":"2023-04-03T08:07:15.065109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Separating positive and negative tweets","metadata":{}},{"cell_type":"code","source":"data_pos = data[data['label'] == 1]\ndata_neg = data[data['label'] == 0]","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.067486Z","iopub.execute_input":"2023-04-03T08:07:15.067842Z","iopub.status.idle":"2023-04-03T08:07:15.168447Z","shell.execute_reply.started":"2023-04-03T08:07:15.067809Z","shell.execute_reply":"2023-04-03T08:07:15.167498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### taking one fourth data so we can run on our machine easily ","metadata":{}},{"cell_type":"code","source":"data_pos = data_pos.iloc[:int(20000)]\ndata_neg = data_neg.iloc[:int(20000)]","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.169595Z","iopub.execute_input":"2023-04-03T08:07:15.169896Z","iopub.status.idle":"2023-04-03T08:07:15.175035Z","shell.execute_reply.started":"2023-04-03T08:07:15.169867Z","shell.execute_reply":"2023-04-03T08:07:15.174154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Combining positive and negative tweets","metadata":{}},{"cell_type":"code","source":"data = pd.concat([data_pos, data_neg])","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.176355Z","iopub.execute_input":"2023-04-03T08:07:15.176637Z","iopub.status.idle":"2023-04-03T08:07:15.206969Z","shell.execute_reply.started":"2023-04-03T08:07:15.176609Z","shell.execute_reply":"2023-04-03T08:07:15.205999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making statement text in lower case","metadata":{}},{"cell_type":"code","source":"data['text']=data['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.208452Z","iopub.execute_input":"2023-04-03T08:07:15.208755Z","iopub.status.idle":"2023-04-03T08:07:15.243452Z","shell.execute_reply.started":"2023-04-03T08:07:15.208726Z","shell.execute_reply":"2023-04-03T08:07:15.242469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.245207Z","iopub.execute_input":"2023-04-03T08:07:15.245653Z","iopub.status.idle":"2023-04-03T08:07:15.256487Z","shell.execute_reply.started":"2023-04-03T08:07:15.245604Z","shell.execute_reply":"2023-04-03T08:07:15.255624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing Stop words of english","metadata":{}},{"cell_type":"code","source":"stopwords_list = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.258042Z","iopub.execute_input":"2023-04-03T08:07:15.258676Z","iopub.status.idle":"2023-04-03T08:07:15.270584Z","shell.execute_reply.started":"2023-04-03T08:07:15.25864Z","shell.execute_reply":"2023-04-03T08:07:15.269641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.272091Z","iopub.execute_input":"2023-04-03T08:07:15.273049Z","iopub.status.idle":"2023-04-03T08:07:15.28871Z","shell.execute_reply.started":"2023-04-03T08:07:15.272988Z","shell.execute_reply":"2023-04-03T08:07:15.287079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing the above stop words list from the tweet text","metadata":{}},{"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\ndef cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndata['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\ndata['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.290786Z","iopub.execute_input":"2023-04-03T08:07:15.291649Z","iopub.status.idle":"2023-04-03T08:07:15.453946Z","shell.execute_reply.started":"2023-04-03T08:07:15.2916Z","shell.execute_reply":"2023-04-03T08:07:15.452724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing punctuations ","metadata":{}},{"cell_type":"code","source":"english_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.456165Z","iopub.execute_input":"2023-04-03T08:07:15.456685Z","iopub.status.idle":"2023-04-03T08:07:15.463299Z","shell.execute_reply.started":"2023-04-03T08:07:15.456636Z","shell.execute_reply":"2023-04-03T08:07:15.462154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\ndata['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.465156Z","iopub.execute_input":"2023-04-03T08:07:15.46592Z","iopub.status.idle":"2023-04-03T08:07:15.663982Z","shell.execute_reply.started":"2023-04-03T08:07:15.465874Z","shell.execute_reply":"2023-04-03T08:07:15.662963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing repeating characters","metadata":{}},{"cell_type":"code","source":"def cleaning_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.665703Z","iopub.execute_input":"2023-04-03T08:07:15.66603Z","iopub.status.idle":"2023-04-03T08:07:15.670391Z","shell.execute_reply.started":"2023-04-03T08:07:15.665998Z","shell.execute_reply":"2023-04-03T08:07:15.669342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\ndata['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:15.67189Z","iopub.execute_input":"2023-04-03T08:07:15.672418Z","iopub.status.idle":"2023-04-03T08:07:16.044609Z","shell.execute_reply.started":"2023-04-03T08:07:15.672365Z","shell.execute_reply":"2023-04-03T08:07:16.043726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing email","metadata":{}},{"cell_type":"code","source":"def cleaning_email(data):\n    return re.sub('@[^\\s]+', ' ', data)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.045876Z","iopub.execute_input":"2023-04-03T08:07:16.04642Z","iopub.status.idle":"2023-04-03T08:07:16.051278Z","shell.execute_reply.started":"2023-04-03T08:07:16.046362Z","shell.execute_reply":"2023-04-03T08:07:16.050171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']= data['text'].apply(lambda x: cleaning_email(x))\ndata['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.052987Z","iopub.execute_input":"2023-04-03T08:07:16.053347Z","iopub.status.idle":"2023-04-03T08:07:16.128762Z","shell.execute_reply.started":"2023-04-03T08:07:16.05329Z","shell.execute_reply":"2023-04-03T08:07:16.127849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing URL's","metadata":{}},{"cell_type":"code","source":"def cleaning_URLs(data):\n    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.130077Z","iopub.execute_input":"2023-04-03T08:07:16.130626Z","iopub.status.idle":"2023-04-03T08:07:16.135607Z","shell.execute_reply.started":"2023-04-03T08:07:16.13057Z","shell.execute_reply":"2023-04-03T08:07:16.134287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\ndata['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.137554Z","iopub.execute_input":"2023-04-03T08:07:16.137889Z","iopub.status.idle":"2023-04-03T08:07:16.317718Z","shell.execute_reply.started":"2023-04-03T08:07:16.137856Z","shell.execute_reply":"2023-04-03T08:07:16.316858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning and removing Numeric numbers","metadata":{}},{"cell_type":"code","source":"def cleaning_numbers(data):\n    return re.sub('[0-9]+', '', data)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.31895Z","iopub.execute_input":"2023-04-03T08:07:16.319441Z","iopub.status.idle":"2023-04-03T08:07:16.323331Z","shell.execute_reply.started":"2023-04-03T08:07:16.319386Z","shell.execute_reply":"2023-04-03T08:07:16.322487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\ndata['text'].tail()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.329364Z","iopub.execute_input":"2023-04-03T08:07:16.32994Z","iopub.status.idle":"2023-04-03T08:07:16.452657Z","shell.execute_reply.started":"2023-04-03T08:07:16.329742Z","shell.execute_reply":"2023-04-03T08:07:16.451414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Getting tokenization of tweet text","metadata":{}},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\ndata['text'] = data['text'].apply(tokenizer.tokenize)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.454097Z","iopub.execute_input":"2023-04-03T08:07:16.454479Z","iopub.status.idle":"2023-04-03T08:07:16.597642Z","shell.execute_reply.started":"2023-04-03T08:07:16.454445Z","shell.execute_reply":"2023-04-03T08:07:16.59664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.600376Z","iopub.execute_input":"2023-04-03T08:07:16.600837Z","iopub.status.idle":"2023-04-03T08:07:16.614218Z","shell.execute_reply.started":"2023-04-03T08:07:16.600789Z","shell.execute_reply":"2023-04-03T08:07:16.612684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying Stemming","metadata":{}},{"cell_type":"code","source":"st = nltk.PorterStemmer()\ndef stemming_on_text(data):\n    text = [st.stem(word) for word in data]\n    return data\n\ndata['text']= data['text'].apply(lambda x: stemming_on_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:16.615591Z","iopub.execute_input":"2023-04-03T08:07:16.615994Z","iopub.status.idle":"2023-04-03T08:07:24.781361Z","shell.execute_reply.started":"2023-04-03T08:07:16.615961Z","shell.execute_reply":"2023-04-03T08:07:24.780017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:24.783012Z","iopub.execute_input":"2023-04-03T08:07:24.783363Z","iopub.status.idle":"2023-04-03T08:07:24.792087Z","shell.execute_reply.started":"2023-04-03T08:07:24.783331Z","shell.execute_reply":"2023-04-03T08:07:24.790904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying Lemmatizer","metadata":{}},{"cell_type":"code","source":"lm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(data):\n    text = [lm.lemmatize(word) for word in data]\n    return data\n\ndata['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:24.793309Z","iopub.execute_input":"2023-04-03T08:07:24.793598Z","iopub.status.idle":"2023-04-03T08:07:27.893919Z","shell.execute_reply.started":"2023-04-03T08:07:24.79357Z","shell.execute_reply":"2023-04-03T08:07:27.892678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:27.895414Z","iopub.execute_input":"2023-04-03T08:07:27.895832Z","iopub.status.idle":"2023-04-03T08:07:27.90412Z","shell.execute_reply.started":"2023-04-03T08:07:27.895799Z","shell.execute_reply":"2023-04-03T08:07:27.903292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> <h3> Labels : </h3></b>  labels are the targets like in this project senitments of the tweets are labels. ","metadata":{}},{"cell_type":"markdown","source":"<b> <h3> Inputs : </h3></b>  Inputs are the data that we feed into machine learning like in this project tweets texts are the inputs. ","metadata":{}},{"cell_type":"markdown","source":"<b> <h3> Training Data </h3></b>  We use training data when we train the models. We feed train data to machine learning and deep learning models so that model can learn from the data.","metadata":{}},{"cell_type":"markdown","source":"<b> <h3> Validation Data </h3></b>  We use validation data while training the model. We use this data to evalaute the performance that how the model perform on training time.","metadata":{}},{"cell_type":"markdown","source":"<b> <h3> Testing Data </h3></b>  We use testing data after training the model. We use this data to evalaute the performance that how the model perform after training. So in this way first we get predictions from the trained model without giving the labels and then we compare the true labels with predictions and get the performance of th model..","metadata":{}},{"cell_type":"markdown","source":"####  Separating input feature and label","metadata":{}},{"cell_type":"code","source":"X=data.text\ny=data.label","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:27.905517Z","iopub.execute_input":"2023-04-03T08:07:27.90614Z","iopub.status.idle":"2023-04-03T08:07:27.913936Z","shell.execute_reply.started":"2023-04-03T08:07:27.906108Z","shell.execute_reply":"2023-04-03T08:07:27.912979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preparing the input features for training \n- We converting the text words into arrays form. \n- Maximum 500 features/words selected for training. These 500 words will be selected on the importance that will distinguish between the positive tweets and negative tweets. ","metadata":{}},{"cell_type":"code","source":"max_len = 500\ntok = Tokenizer(num_words=2000)\ntok.fit_on_texts(X)\nsequences = tok.texts_to_sequences(X)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{"id":"plIFObh4jYJV","outputId":"cc21833e-e053-454b-d410-b8e9d48b4392","execution":{"iopub.status.busy":"2023-04-03T08:07:27.915064Z","iopub.execute_input":"2023-04-03T08:07:27.915589Z","iopub.status.idle":"2023-04-03T08:07:28.795463Z","shell.execute_reply.started":"2023-04-03T08:07:27.915552Z","shell.execute_reply":"2023-04-03T08:07:28.794447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see that there total 40000 tweets and the number words/features are 500.","metadata":{}},{"cell_type":"code","source":"sequences_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:28.796659Z","iopub.execute_input":"2023-04-03T08:07:28.797125Z","iopub.status.idle":"2023-04-03T08:07:28.802844Z","shell.execute_reply.started":"2023-04-03T08:07:28.797093Z","shell.execute_reply":"2023-04-03T08:07:28.801436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Separating the 70% data for training data and 30% for testing data","metadata":{}},{"cell_type":"markdown","source":"As we prepared all the tweets, now we are separating/splitting the tweets into training data and testing data.\n- 70% tweets will be used in the training \n- 30% tweets will be used to test the performance of the model.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)","metadata":{"id":"oOlkTTg4oRqR","execution":{"iopub.status.busy":"2023-04-03T08:07:28.804538Z","iopub.execute_input":"2023-04-03T08:07:28.804947Z","iopub.status.idle":"2023-04-03T08:07:28.851833Z","shell.execute_reply.started":"2023-04-03T08:07:28.8049Z","shell.execute_reply":"2023-04-03T08:07:28.850793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Implementing Tensorflow based model for training üß™</strong></center></h2>   \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h4> Step 1</h4>\n- The input to model is 500 words because these are the number features/words that we extracted above from text of tweets.\n\n<h4> Step 2</h4>\n- Embeddings provide the presentation of words and their relative meanings. Like in this, we are feeding the limit of maximum words, lenght of input words and the inputs of previous layer. \n\n<h4> Step 3</h4>\n- LSTM (long short term memory) save the words and predict the next words based on the previous words. LSTM is a sequance predictor of next coming words.\n\n<img src=\"https://static.wixstatic.com/media/3eee0b_969c1d3e8d7943f0bd693d6151199f69~mv2.gif\">\n<h4> Ref: https://static.wixstatic.com/media/3eee0b_969c1d3e8d7943f0bd693d6151199f69~mv2.gif </h4>\n\n<h4> Step 4</h4>\n- Dense layer reduce the outputs by getting inputs from Faltten layer. Dense layer use all the inputs of previous layer neurons and perform calculations and send 256 outputs\n\n<h4> Step 5</h4>\n- Activation function is node that is put at the end of all layers of neural network model or in between neural network layers. Activation function help to decide which neuron should be pass and which neuron should fire. So activation function of node defines the output of that node given an input or set of inputs. \n\n<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*mcJfRvd9zarAbkHppFRrCQ.gif\">\n<h4> Ref: https://miro.medium.com/v2/resize:fit:1400/1*mcJfRvd9zarAbkHppFRrCQ.gif </h4>\n\n<h4> Step 6</h4>\n- Droupout layer drop some neurons from previous layers. why we apply this? We apply this to avoid the overfitting problems. In overfitting, model give good accuracy on training time but not good on testing time.\n<img src=\"https://i.imgur.com/a86utxY.gif\">\n<h4> Ref: https://i.imgur.com/a86utxY.gif </h4>","metadata":{}},{"cell_type":"code","source":"def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n    inputs = Input(name='inputs',shape=[max_len])#step1\n    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n    layer = LSTM(64)(layer) #step3\n    layer = Dense(256,name='FC1')(layer) #step4\n    layer = Activation('relu')(layer) # step5\n    layer = Dropout(0.5)(layer) # step6\n    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n    return model #function returning the value when we call it","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:28.853275Z","iopub.execute_input":"2023-04-03T08:07:28.853569Z","iopub.status.idle":"2023-04-03T08:07:28.860928Z","shell.execute_reply.started":"2023-04-03T08:07:28.853539Z","shell.execute_reply":"2023-04-03T08:07:28.859834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model compilation","metadata":{}},{"cell_type":"markdown","source":"- First we are calling the model\n- We are using 2 classes so we set \"binary_crossentropy\" and if we use more than two classes then we use \"categorical_crossentropy\" \n- Optimizer is a function that used to change the features of neural network such as learning rate (how the model learn with features) in order to reduce the losses. So the learning rate of neural network to reduce the losses is defined by optimizer.\n- We are setting metrics=accuracy because we are going to caluclate the percentage of correct predictions over all predictions on the validation set","metadata":{}},{"cell_type":"code","source":"model = tensorflow_based_model() # here we are calling the function of created model\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  ","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:28.862457Z","iopub.execute_input":"2023-04-03T08:07:28.862755Z","iopub.status.idle":"2023-04-03T08:07:29.197013Z","shell.execute_reply.started":"2023-04-03T08:07:28.862726Z","shell.execute_reply":"2023-04-03T08:07:29.196099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training and validating with parameter tuning\n- We are feeding the training data and getting 10% data for validation from training data\n* We set the following parameters:\n- Batch size =80 so the model take 80 tweets in each iteration and train them. Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. \n- Epochs =6 so the model will train on the data 6 times.Epoch is a term used in machine learning and indicates the number of passes of the entire training dataset the machine learning algorithm has completed. \n- We can choose batch_size, and epochs as we want so the good practice is to set some values and train the model if the model will not give the good results we can change it and then try again for the training of the model. We can repeat this process many time untill we will not get the good results and this process called as parameter tuning.\n","metadata":{}},{"cell_type":"code","source":"history=model.fit(X_train,Y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\nprint('Training finished !!')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:07:29.198154Z","iopub.execute_input":"2023-04-03T08:07:29.198483Z","iopub.status.idle":"2023-04-03T08:21:48.133043Z","shell.execute_reply.started":"2023-04-03T08:07:29.198452Z","shell.execute_reply":"2023-04-03T08:21:48.131766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1> We need to do all the above configurations to train the model. If we will not set all settings correctly then we could not get the desired results.</h1>","metadata":{}},{"cell_type":"markdown","source":"#### Testing the Trained model on test data\n- Getting predictions/classifying the sentiments (positve/negative) on the test data using trained model.","metadata":{"id":"aQ72A5F4U0wr"}},{"cell_type":"code","source":"accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here","metadata":{"executionInfo":{"elapsed":2136,"status":"ok","timestamp":1590595289545,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"LlxD3pU9U0ws","outputId":"a1adf83f-f07c-4f10-fc1c-a895f6e39d56","execution":{"iopub.status.busy":"2023-04-03T08:21:48.135145Z","iopub.execute_input":"2023-04-03T08:21:48.136111Z","iopub.status.idle":"2023-04-03T08:22:16.748346Z","shell.execute_reply.started":"2023-04-03T08:21:48.13606Z","shell.execute_reply":"2023-04-03T08:22:16.747021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy\n- Accuracy is the number of correctly classify tweets from all the tweets of positive and negative. \n- For example, if the trained model classify the 70 tweets correct and 30 tweets wrong from total of 100 tweets then the accuracy score will be 70%. \n- Accuracy= Total number of correct predictions/Total number of predictions","metadata":{"id":"h5ZOTOh14WKO"}},{"cell_type":"code","source":"print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below","metadata":{"executionInfo":{"elapsed":3304,"status":"ok","timestamp":1590596501745,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"E2bQq4jaU0wt","outputId":"fe61d1b5-954a-4770-d423-ab897f64b495","execution":{"iopub.status.busy":"2023-04-03T08:22:16.750445Z","iopub.execute_input":"2023-04-03T08:22:16.750791Z","iopub.status.idle":"2023-04-03T08:22:16.758308Z","shell.execute_reply.started":"2023-04-03T08:22:16.75076Z","shell.execute_reply":"2023-04-03T08:22:16.756728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Getting prediction of the test data and then we will compare the true labels/classes of the data with predictions\n- As the model give probabilties so we are setting a threshold 0.5. More than 0.5 will be the positive tweets and lower will be negative tweets","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test) #getting predictions on the trained model\ny_pred = (y_pred > 0.5) ","metadata":{"execution":{"iopub.status.busy":"2023-04-03T08:22:16.761132Z","iopub.execute_input":"2023-04-03T08:22:16.762381Z","iopub.status.idle":"2023-04-03T08:22:42.699811Z","shell.execute_reply.started":"2023-04-03T08:22:16.76233Z","shell.execute_reply":"2023-04-03T08:22:42.698716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion matrix\n- These are the evaluation measures to evaluate the performance of the model.\n- Dark blue boxes are the correct predictions with the trained model and sky blue boxes shows the wrong predictions.\n- 4610 tweets correctly predicted as negative sentiments. 1399 tweets predicted positive sentiments but that were actually negative sentiments.\n- 4247 tweets correctly predicted as postive sentiments. 1744 tweets predicted negative sentiments but that were actually positive sentiments.","metadata":{"id":"40MjbuBA7Og9"}},{"cell_type":"code","source":"print('\\n')\nprint(\"confusion matrix\")\nprint('\\n')\nCR=confusion_matrix(Y_test, y_pred)\nprint(CR)\nprint('\\n')\n\nfig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","metadata":{"executionInfo":{"elapsed":17851,"status":"ok","timestamp":1590596595973,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"WERD7KXs8YmQ","outputId":"db07021b-ec87-4be8-ce17-14cffe8749a6","execution":{"iopub.status.busy":"2023-04-03T08:22:42.701316Z","iopub.execute_input":"2023-04-03T08:22:42.701958Z","iopub.status.idle":"2023-04-03T08:22:43.004857Z","shell.execute_reply.started":"2023-04-03T08:22:42.701924Z","shell.execute_reply":"2023-04-03T08:22:43.003346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC CURVE\n- ROC curve show the performance of the model as well. \n- We can see that the model started from the 0 percent predictions and then moved to true positive predictions that are correct\n- ROC curve (receiver operating characteristic curve) show the performance of a classification model at all the classification thresholds. ROC plots two parameters, True Positive Rate (correct predictions/classifications) False Positive Rate (wrong predictions/classifications)","metadata":{"id":"wlWpCx_OXEIq"}},{"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"executionInfo":{"elapsed":1346,"status":"ok","timestamp":1589235307230,"user":{"displayName":"Uƒüur Mertoƒülu","photoUrl":"","userId":"13630859382453662878"},"user_tz":-180},"id":"uypTDMUZXEIq","outputId":"f18a045e-3bbc-4b9b-afbf-29db50e9cbfd","scrolled":true,"execution":{"iopub.status.busy":"2023-04-03T08:22:43.006745Z","iopub.execute_input":"2023-04-03T08:22:43.00723Z","iopub.status.idle":"2023-04-03T08:22:43.179625Z","shell.execute_reply.started":"2023-04-03T08:22:43.007147Z","shell.execute_reply":"2023-04-03T08:22:43.17843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n<h1><center><strong>Conclusion üìù</strong></center></h1>\n    <p>\n<li>We used the twitter sentiment analysis dataset and explored the data with different ways.</li>\n        <li>We prepared the text data of tweets by removing the unnecessary things.</li>\n          <li>We trained model based on tensorflow with all settings. </li>\n        <li>We evaluated thye model with different evaluation measures.</li>\n         <li>If you are interested to work on any text based project, you can simply apply the same methodolgy but might be you will need to change little settings like name of coloumns etc.</li>\n        <li>We worked on the classification problem and sepcifically we call it binary classification which is two class classification.</li>\n        </p>\n</div>","metadata":{}}]}